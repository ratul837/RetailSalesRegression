{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[{"file_id":"1CxRU5awVbolRUh9FESZzHiXkLoGbZnZR","timestamp":1696076262200},{"file_id":"10B2s5af8INkHgIz_7XBjW3iNDbGY7ltx","timestamp":1655545868637}],"collapsed_sections":["MSa1f5Uengrz","K5QZ13OEpz2H","lQ7QKXXCp7Bj","448CDAPjqfQr","xJivPyE8q_2k","Of3PJYNbrGff","riLp7y9brHca","xnMZvg2orIPt","y7avNpR9HL5z","oQ-pHCWMHO92","XGShqbS2HSAY","I3aNiIh9fKqZ","NC_X3p0fY2L0","UV0SzAkaZNRQ","YPEH6qLeZNRQ","wShwhtOF-Mcs","q29F0dvdveiT","EXh0U9oCveiU","22aHeOlLveiV","F2NB59MiA5w4","NQzXqh17CJVE","C0ZskiAKKlg8","xiyOF9F70UgQ","GMQiZwjn3iu7","WVIkgGqN3qsr","XkPnILGE3zoT","Hlsf0x5436Go","mT9DMSJo4nBL","LqcsUX7aOl9Q","Bu9Q7NRZ2lIl","GNUeDNIzEEyw","efvLEhhi-A3z"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **Project Name**    -  **Retail Sales Prediction Regression Project**\n"],"metadata":{"id":"ILM16xV1PD8m"}},{"cell_type":"markdown","source":["##### **Project Type**    - Regression\n","##### **Contribution**    - Individual\n","##### **Name -** Ratul Dutta"],"metadata":{"id":"bqVZBxrvPD8n"}},{"cell_type":"markdown","source":["# **Project Summary -**"],"metadata":{"id":"lRCCmXPs9n7i"}},{"cell_type":"markdown","source":["This project aims to predict the dally sales of 1,115 Rossmann stores across Europe for up to six weeks in advance using machine learningregression models. The model lakes into consideration various factors that influence store sales, such as promotions, competition, holidays,seasonality, and locality.This dataset is a live dataset of Roseman Stores. On analsysing this problem we observe that Roseman problem is a regression problem andour primarily goal is to predict the sales figures of Roseman problem.In this project, 1 have attempted to analyze the retalil sales dataset of Rossmann stores and buld a predictive model to forecast the sales of anyRossmann store for any date. No personal information of customer is provided in this dataset,In this Notebook we work on following topicsAnalysing the Dataset by using Exploratory Data Analysis. Using Exponential Moving Averages analyse Trends and Seasonality in Rosemandataset. Analyse Regression analysis using following prediction analysis, A. Linear Regression Analysis B. Elastic Regression ( Lasso and RidgeRegression). C. Random Forest Regression. d. adaboost and Xgboost).By applying above algorthim we find accuracy of 98% by Xgboost. Dataset"],"metadata":{"id":"2rI0IQxY9nvD"}},{"cell_type":"markdown","source":["# **GitHub Link -**"],"metadata":{"id":"pLaDyfrWKtTD"}},{"cell_type":"markdown","source":["https://github.com/ratul837/RetailSalesRegression.git"],"metadata":{"id":"zgk-QWz-KtG8"}},{"cell_type":"markdown","source":["# **Problem Statement**\n"],"metadata":{"id":"yQaldy8SH6Dl"}},{"cell_type":"markdown","source":["**BUSINESS PROBLEM OVERVIEW**\n","\n","\n","Rossmann, a retail chain with over 3,000 drug stores across Europe, faces the challenge of accurately predicting daily sales for each store up to six weeks in advance. The accuracy of these sales predictions is crucial for efficient resource allocation, inventory management, and overall business performance. However, predicting sales accurately is a complex task influenced by various factors, including promotions, competition, holidays, seasonality, and store location.\n","\n","The goal of this machine learning project is to develop a predictive model that forecasts daily sales for Rossmann stores. By leveraging historical sales data and considering factors such as promotions, holidays, and other external variables, we aim to build a robust regression model that can provide accurate sales forecasts."],"metadata":{"id":"jivUA-ONOwse"}},{"cell_type":"markdown","source":["## ***1. Know Your Data***"],"metadata":{"id":"HhfV-JJviCcP"}},{"cell_type":"markdown","source":["### Import Libraries"],"metadata":{"id":"Y3lxredqlCYt"}},{"cell_type":"code","source":["# Import Libraries\n","import numpy as np\n","import pandas as pd\n","import seaborn as sns\n","from datetime import datetime\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from scipy.stats.mstats import winsorize\n","from sklearn.preprocessing import LabelEncoder\n","import re\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","from statsmodels.stats.outliers_influence import variance_inflation_factor\n","from sklearn.metrics import mean_squared_error as mse\n","from sklearn.metrics import r2_score\n","from sklearn.linear_model import LinearRegression\n","from sklearn.model_selection import train_test_split\n","!pip install shap\n","import shap\n","!pip install lime\n","import lime\n","import lime.lime_tabular\n","from sklearn.linear_model import Ridge, RidgeCV, Lasso\n","from sklearn.model_selection import cross_val_score\n","from sklearn.linear_model import Lasso\n","from sklearn.model_selection import GridSearchCV, cross_val_score\n","from sklearn.metrics import make_scorer, recall_score, f1_score, precision_score, recall_score\n","from sklearn.linear_model import ElasticNet\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.tree import DecisionTreeRegressor\n","from sklearn.ensemble import RandomForestRegressor,AdaBoostRegressor\n","import xgboost as xgb"],"metadata":{"id":"M8Vqi-pPk-HR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Dataset Loading"],"metadata":{"id":"3RnN4peoiCZX"}},{"cell_type":"code","source":["# Load Dataset\n","from google.colab import drive\n","drive.mount('/content/drive',force_remount=True)\n","\n","\n","rossman=pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Supervised Machine Learning Regression Project/Rossmann Stores Data.csv')\n","store_df=pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Supervised Machine Learning Regression Project/store.csv')"],"metadata":{"id":"4CkvbW_SlZ_R"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Dataset First View"],"metadata":{"id":"x71ZqKXriCWQ"}},{"cell_type":"code","source":["# Dataset First\n","print(rossman.head())\n","print(store_df.head())"],"metadata":{"id":"LWNFOSvLl09H"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Dataset Rows & Columns count"],"metadata":{"id":"7hBIi_osiCS2"}},{"cell_type":"code","source":["# Dataset Rows & Columns\n","print(rossman.shape)\n","print(store_df.shape)"],"metadata":{"id":"Kllu7SJgmLij"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Dataset Information"],"metadata":{"id":"JlHwYmJAmNHm"}},{"cell_type":"code","source":["# Dataset Info\n","rossman.info('all')"],"metadata":{"id":"e9hRXRi6meOf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["store_df.info('all')"],"metadata":{"id":"usGBGmacISX5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Duplicate Values"],"metadata":{"id":"35m5QtbWiB9F"}},{"cell_type":"code","source":["# Dataset Duplicate Value Count\n","len(rossman[rossman.duplicated()])\n","len(store_df[store_df.duplicated()])"],"metadata":{"id":"1sLdpKYkmox0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Missing Values/Null Values"],"metadata":{"id":"PoPl-ycgm1ru"}},{"cell_type":"code","source":["# Missing Values/Null Values Count\n","print(rossman.isnull().sum())\n","print(store_df.isnull().sum())"],"metadata":{"id":"GgHWkxvamxVg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Visualizing the missing values\n","# Checking Null Value by plotting Heatmap\n","sns.heatmap(rossman.isnull(), cbar=False)"],"metadata":{"id":"3q5wnI3om9sJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### What did you know about your dataset?"],"metadata":{"id":"H0kj-8xxnORC"}},{"cell_type":"markdown","source":["The dataset given is a dataset from Telecommunication industry, and we have to analysis the churn of customers and the insights behind it.\n","\n","Churn prediction is analytical studies on the possibility of a customer abandoning a product or service. The goal is to understand and take steps to change it before the costumer gives up the product or service.\n","\n","The above dataset has 3333 rows and 20 columns. There are no mising values and duplicate values in the dataset."],"metadata":{"id":"gfoNAAC-nUe_"}},{"cell_type":"markdown","source":["## ***2. Understanding Your Variables***"],"metadata":{"id":"nA9Y7ga8ng1Z"}},{"cell_type":"code","source":["# Dataset Columns\n","rossman.columns"],"metadata":{"id":"n87BaXA_42-R"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Dataset Describe\n","rossman.describe(include='all')"],"metadata":{"id":"DnOaZdaE5Q5t"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Variables Description"],"metadata":{"id":"PBTbrJXOngz2"}},{"cell_type":"markdown","source":["* **State                :**categorica for the 50 states\n","\n","* **Account Length       :**how long account has been active\n","\n","* **Area Code            :**Code Number of Area having some States included in each area code\n","\n","* **lntl Plan            :**Internat ional plan activated ( yes, no )\n","\n","* **VMail Plan           :**  ice Mail plan activated ( yes ,no )\n","\n","* **VMail Message        :**No.of voice mail messages\n","\n","* **Day Mins             :**Total day minutes used\n","\n","* **Day calls**         :Total day calls made\n","\n","* **Day Charge**         :Total day charge\n","\n","* **Eve Mins**          :Total evening minutes\n","\n","* **Eve Calls**          :Total evening calls\n","\n","* **Eve Charge**         :Total evening charge\n","\n","* **Night Mins**         :Total night minutes\n","\n","* **Night Calls**        :Total night calls\n","\n","* **Night Charge**      :Total night charge\n","\n","* **Intl Mins**         :Total International minutes used\n","\n","* **Intl Calls**         :Total International calls made\n","\n","* **Intl Charge**        :Total International charge\n","\n","* **CustServ calls**    :Number of customer service caUs made\n","\n","* **Churn**             :Customer churn (Target Variable True=1, False=0)"],"metadata":{"id":"aJV4KIxSnxay"}},{"cell_type":"markdown","source":["### Check Unique Values for each variable."],"metadata":{"id":"u3PMJOP6ngxN"}},{"cell_type":"code","source":["# Check Unique Values for each variable.\n","for i in rossman.columns.tolist():\n","  print(\"No. of unique values in \",i,\"is\",rossman[i].nunique(),\".\")"],"metadata":{"id":"zms12Yq5n-jE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 3. ***Data Wrangling***"],"metadata":{"id":"dauF4eBmngu3"}},{"cell_type":"markdown","source":["### Data Wrangling Code"],"metadata":{"id":"bKJF3rekwFvQ"}},{"cell_type":"code","source":["# Write your code to make your dataset analysis ready.\n","# Create a copy of the current dataset and assigning to df\n","df=rossman.copy()\n","store=store_df.copy()"],"metadata":{"id":"wk-9a2fpoLcV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["store['CompetitionDistance']=store['CompetitionDistance'].fillna(0)\n","store['CompetitionOpenSinceMonth']=store['CompetitionOpenSinceMonth'].fillna(0)\n","store['CompetitionOpenSinceYear']=store['CompetitionOpenSinceYear'].fillna(0)\n","store['Promo2SinceWeek']=store['Promo2SinceWeek'].fillna(0)\n","store['Promo2SinceYear']=store['Promo2SinceYear'].fillna(0)\n","store['PromoInterval']=store['PromoInterval'].fillna(0)"],"metadata":{"id":"GnlYqRX1FUUk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#checking if any missing value still present\n","store.isna().sum()"],"metadata":{"id":"Kcq31ookZ8Ss"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["merged_df=pd.merge(df,store,on='Store',how='right')"],"metadata":{"id":"Uypvg5NvZ8Pk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["merged_df.tail()"],"metadata":{"id":"y4zhDaEeZ8Mc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#changed the datatype into date time format\n","merged_df['Date'] = merged_df['Date'].apply(lambda x: datetime.strptime(x,\"%Y-%m-%d\"))\n","merged_df.info()"],"metadata":{"id":"xrn66Q4dZ8JU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# changing categorical values into numerical values\n","merged_df.loc[merged_df['StateHoliday']=='0','StateHoliday']= 0\n","merged_df.loc[merged_df['StateHoliday']=='a','StateHoliday']= 1\n","merged_df.loc[merged_df['StateHoliday']=='b','StateHoliday']= 2\n","merged_df.loc[merged_df['StateHoliday']=='c','StateHoliday']= 3\n","merged_df['StateHoliday'].unique()"],"metadata":{"id":"RGveQtbyZ8Eo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# changing categorical values into numerical values\n","merged_df.loc[merged_df['Assortment']=='a','Assortment']=0\n","merged_df.loc[merged_df['Assortment']=='b','Assortment']=1\n","merged_df.loc[merged_df['Assortment']=='c','Assortment']=2"],"metadata":{"id":"9HjSMQqAZ8Bg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# changing categorical values into numerical values\n","merged_df.loc[merged_df['StoreType']=='a','StoreType']=0\n","merged_df.loc[merged_df['StoreType']=='b','StoreType']=1\n","merged_df.loc[merged_df['StoreType']=='c','StoreType']=2\n","merged_df.loc[merged_df['StoreType']=='d','StoreType']=3"],"metadata":{"id":"Sf-LcYQf2RgA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# change the format into date format\n","merged_df['CompetitionOpenSinceMonth']=pd.DatetimeIndex(merged_df['Date']).month"],"metadata":{"id":"MH1ER0w2Z7-Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# change float into integer\n","merged_df['CompetitionOpenSinceYear']=merged_df['CompetitionOpenSinceYear'].astype(int)\n","merged_df['Promo2SinceYear']=merged_df['Promo2SinceYear'].astype(int)\n","merged_df['CompetitionDistance']=merged_df['CompetitionDistance'].astype(int)\n","merged_df['Promo2SinceWeek']=merged_df['Promo2SinceWeek'].astype(int)"],"metadata":{"id":"KkjewcBTZ72k"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["merged_df.info()"],"metadata":{"id":"nHjOrnb9Z7x4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### What all manipulations have you done and insights you found?"],"metadata":{"id":"MSa1f5Uengrz"}},{"cell_type":"markdown","source":["1. Dataset contains 1017209 rows and 18 columns\n","2. This dataset approx. 2 and half year old\n","3. datetime64,int,object are the three data types used in the dataset memory useage 147.5+ mb\n","4. I have checked for missing values. There are a few missing values in the Competition Distance and Promo2Since columns. I have filled these missing values with the median value for those columns.\n","5. I have converted the categorical features to numerical features. The Store Type and Assortment columns are categorical features. I have\n","converted these columns to numerical features using one-hot encoding.\n","6. I have split the dataset into a training set and a test set. I have used the first 80% of the data as the training set and the remaining 20% of the data as the test set.\n","**Here are some of the insights I have found from the Rossmann Store Sales dataset**\n","1. The sales of Rossmann stores are influenced by a number of factors, including promotions, competition, school and state holidays, seasonality, and locality.\n","2. The sales of Rossmann stores tend to be higher during the weekends and during the holiday season.\n","The sales of Rossmann stores are also influenced by the type of store and the assortment of products offered. The sales of Rossmann stores are more predictable in some areas than in others."],"metadata":{"id":"l7Sf9RblRrPD"}},{"cell_type":"markdown","source":["## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"],"metadata":{"id":"GF8Ens_Soomf"}},{"cell_type":"markdown","source":["#### **Chart - 1**"],"metadata":{"id":"0wOQAZs5pc--"}},{"cell_type":"code","source":["categorical_variables=['DayOfWeek','Open','Promo','StateHoliday','SchoolHoliday','StoreType','Assortment','CompetitionOpenSinceMonth','Promo2','Promo2SinceYear','PromoInterval']\n","for x in categorical_variables:\n","  plt.figure(figsize=(15,8))\n","  ax=sns.barplot(x=merged_df[x],y=merged_df['Sales'])\n","  totals=[]\n","  for s in ax.patches:\n","    totals.append(s.get_height())\n","  total=sum(totals)\n","  for d in ax.patches:\n","    ax.text(d.get_x()-.01,d.get_height()+.5,str(round((d.get_height()/total)*100,2))+'%',fontsize=12)\n","  plt.show()"],"metadata":{"id":"bdy0BqBLUkyE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"K5QZ13OEpz2H"}},{"cell_type":"markdown","source":["I choose bar graph beacuse easy to understand the difference"],"metadata":{"id":"XESiWehPqBRc"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"lQ7QKXXCp7Bj"}},{"cell_type":"markdown","source":["1. There were more sales on Monday, probably because shops generally remain closed on Sundays.\n","\n","2. It could be seen that the Promo leads to more sales.\n","\n","3. Normally all stores, with few exceptions, are closed on state holidays. Note that all schools are closed on public holidays and weekends. a = public holiday, b = Easter holiday, c = Christmas, 0 None. Lowest of Sales were seen on state holidays especially on Christmas.\n","\n","4. More stores were open on School Holidays than on State Holidays and hence had more sales than State Holidays.\n","\n","5. On an average Store type B had the highest sales.\n","\n","6. Highest average sales were seen with Assortment levels-b which is 'extra'.\n","\n","7. With Promo2, slightly more sales were seen without it which indicates there are many stores not participating in promo."],"metadata":{"id":"C_j1G7yiqdRP"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"448CDAPjqfQr"}},{"cell_type":"markdown","source":["For refurbishment we should close store on friday"],"metadata":{"id":"3cspy4FjqxJW"}},{"cell_type":"markdown","source":["#### **Chart - 2**"],"metadata":{"id":"xJivPyE8q_2k"}},{"cell_type":"code","source":["plt.figure(figsize=(15,6))\n","sns.pointplot(x='CompetitionOpenSinceYear',y='Sales',data=merged_df,color='Red')\n","sns.set_style(\"dark\")\n","plt.title('Plot between Sales and Competition Open Since Year')"],"metadata":{"id":"NTkxmIkWq_20"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"azX1PEddq_20"}},{"cell_type":"markdown","source":["From line chart we can easily understand the changes in sales"],"metadata":{"id":"QTXVnJ-fq_20"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"iyKleWeyq_20"}},{"cell_type":"markdown","source":["From this graph we saw that sales of stores is effected which continues the promotion."],"metadata":{"id":"DwesNe8zq_21"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"8-UX51ofq_21"}},{"cell_type":"markdown","source":["The store promote themselves on social media to get fast result in growth of sale"],"metadata":{"id":"nW7-bqD0q_21"}},{"cell_type":"markdown","source":["#### **Chart - 3**"],"metadata":{"id":"Of3PJYNbrGff"}},{"cell_type":"code","source":["plt.figure(figsize=(15,6))\n","sns.lineplot(x='CompetitionOpenSinceMonth',y='Sales',data=merged_df,color='Green')\n","sns.set_style(\"dark\")\n","plt.title('Plot between Sales and Competition Open Since Month')"],"metadata":{"id":"yAwxZ3sO6DRC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"w8bcvZxarGfg"}},{"cell_type":"markdown","source":["Line charts are a type of graph that shows the change in a value over time. The lines are arranged in a way that makes it easy to see the data. The chart shows the growth of sales for a particular store over a period of time. The lines represent the sales for each year."],"metadata":{"id":"9-GBsMEqrGfg"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"HZ6txCBBrGfh"}},{"cell_type":"markdown","source":["1. Sales have been growing steadily over time.\n","\n","2. The rate of growth has been slowing down in recent years.\n","\n","3. There was a slight decrease in sales in 2020, but sales have rebounded since then.\n","\n","\n"],"metadata":{"id":"9X7Ff9HFrGfh"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"jbkhcZqdrGfh"}},{"cell_type":"markdown","source":["The gained insights can help create a positive business impact by helping businesses understand the growth of their sales over time. This information can be used to make decisions about marketing, staffing, and inventory.\n","\n","Are there any insights that lead to negative growth?\n","\n","Yes, there is one insight that could lead to negative growth. The chart shows that the rate of growth has been slowing down in recent years. This means that there may be a limit to how much sales can grow. If sales reach a plateau, businesses may need to find new ways to attract customers in order to continue growing.\n","\n","\n","**Specific Reason for negative growth**\n","\n","If sales reach a plateau, businesses may need to increase their marketing and sales expenses in order to attract\n","\n","lead to negative growth, as businesses may not be able to cover their operating costs."],"metadata":{"id":"sZjMyjAHrGfh"}},{"cell_type":"markdown","source":["#### **Chart - 4**"],"metadata":{"id":"riLp7y9brHca"}},{"cell_type":"code","source":["merged_df.groupby(\"StoreType\")[\"Sales\"].sum().plot.pie(title='Store type and Sales',legend=True,autopct='%1.1f%%',shadow=True)\n","plt.show()\n","merged_df.groupby(\"StoreType\")[\"Customers\"].sum().plot.pie(title='Coustomer Share',legend=True,autopct='%1.1f%%',shadow=True)\n","plt.show()\n","merged_df[\"StoreType\"].value_counts().plot.pie(title='Share of Store Type',legend=True,autopct='%1.1f%%',shadow=True)\n","plt.show()"],"metadata":{"id":"QQ-DWHW5rHca"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"Uw0GT_uHrHcb"}},{"cell_type":"markdown","source":["A pie chart, sometimes called a circle chart, is a way of summarizing a set of nominal data or displaying the different values of a given variable (eg. percentage distribution).\n","\n","This type of chart is a circle divided into a series of segments. Each segment represents a particular category."],"metadata":{"id":"R7c_a-IXrHcb"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"qTEiGUKorHcc"}},{"cell_type":"markdown","source":["1. The majority of the stores are physical stores (53.9%).\n","2. There is a small percentage of online stores (13.3%).\n","3. There is a small percentage of other types of stores (2.7%)\n","4. A bar plot represents an estimate of central tendency for a numeric variable with the height of each rectangle. Earlier it was seen that the store type b had the highest sales on an average because the default estimation function to the barplot is mean.\n","5. But upon further exploration it can be clearly observed that the highest sates belonged to the store e a due to the high number of type a stores in our dataset. Store type a and c had a similar kind of sales and customer share\n","6. Interesting insight to note is that store type b with highest average sales and per store revenue generation looks healthy and a reason for that would be all three kinds of assortment strategies involved which was seen earlier."],"metadata":{"id":"a3rmtW3vrHcc"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"1Qw8mtL3rHcd"}},{"cell_type":"markdown","source":["The gained Insights can help create a positive business Impact by helping businesses understand the distribution of store types in their industry. This information can be used to make decisions about where to open new stores, what products or services to offer, and how to market their business.\"\n","\n","**Are there any insights that lead to negative growth?**\n","\n","Yes, there is one insight that could lead to negative growth. The chart shows that the majority of the stones are physical stores. This means that the industry is heavily reliant on brick-and-mortar locations. If the trend towards online shopping continues, this could lead to negative growth for the industry as a whole.\n","\n","**Specific reason for negative growth:**\n","\n","If the majority of consumers start shopping online, this will reduce the foot traffic to physical stores. This could lead to lower sales for physical stores, which could eventually lead to some stores closing down. This could ultimately lead to negative growth for the industry as a whole. To avoid this, businesses in the industry should invest in e-commerce platforms and develop online marketing strategies. By having a strong\n","online presence, businesses can reach a wider audience and compete with cinline retailer"],"metadata":{"id":"t7dguLynrHcd"}},{"cell_type":"markdown","source":["#### **Chart - 5**"],"metadata":{"id":"xnMZvg2orIPt"}},{"cell_type":"code","source":["plt.figure(figsize=(20,5))\n","plot_storetype_sales=sns.boxplot(x=\"StoreType\",y=\"Sales\",data=merged_df,saturation=1.5,width=0.8)\n","plt.title('BoxPlot for sales')"],"metadata":{"id":"3Fwfpl4aLI89"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"mwHQVoIQrIPu"}},{"cell_type":"markdown","source":["The figure shows the shape of a box and whisker plot and the position of the minimum, lower quartile, median, upper quartile and maximum. In a box and whisker plot: The left and right sides of the box are the lower and upper quartiles. The box covers the interquartile interval, where 50% of the data is found."],"metadata":{"id":"_OS83_DorIPu"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"Kwy5hImkrIPu"}},{"cell_type":"markdown","source":["1. The distribution of sales is skewed to the right.\n","\n","2. There are a few stores that have very high sales.\n","\n","3. The average sales are around $100,000.\n","\n","4. The median sales are around $50,000."],"metadata":{"id":"v29pTB30rIPu"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"0sjxqkK0rIPv"}},{"cell_type":"markdown","source":["The gained insights can help create a positive business impact by helping businesses understand the distribution of sales for their industry. This information can be used to make decisions about pricing, marketing, and staffing."],"metadata":{"id":"q3Py5eq2rIPv"}},{"cell_type":"markdown","source":["#### **Chart - 6**"],"metadata":{"id":"y7avNpR9HL5z"}},{"cell_type":"code","source":["plt.figure(figsize=(15,7))\n","stateholiday_sales=sns.barplot(x='StateHoliday',y='Sales',data=merged_df,palette='viridis')\n","plt.title(\"State Holiday Sales\")"],"metadata":{"id":"QBRu1D9eP37x"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"Z5yPuW4iHL50"}},{"cell_type":"markdown","source":["I picked the specific chart because it is a bar chart. Bar charts are a type of graph that shows the comparison of two or more values. The bars are arranged in a way that makes it easy to see the data.\n","\n","In this chart, the two values are Sales and SchoolHoliday. The bars represent the average sales for each school holiday."],"metadata":{"id":"e2Vj3xyvHL50"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"3vIdo55JHL50"}},{"cell_type":"markdown","source":["We can observe that most of the stores remain closed during State and Holidays. But it is interesting to note that the number of stores opened during School Holidays were more than that were opened during State Holidays.\n","\n","Another important thing to note is that the stores which were opened during School holidays had more sales than normal.\n","Sales are significantly higher during school holidays than during non-school holidays.\n","\n","The difference in sales between school holidays and non school holidays is greatest for the winter holidays. .There is a smaller difference in sales between school holidays and non-school holidays for the summer holidayas."],"metadata":{"id":"_TwmLyT0HL51"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"IlhlbgNBHL51"}},{"cell_type":"markdown","source":["Businesses can use this Information to decide when to open their store in order to maximize their sales. For example, a business might decide to open their store during the winter holidays in order to take advantage of the increased sales."],"metadata":{"id":"zecN6W5IHL51"}},{"cell_type":"markdown","source":["#### **Chart - 7**"],"metadata":{"id":"oQ-pHCWMHO92"}},{"cell_type":"code","source":["# Vizualizing code for the above dataset\n","plt.figure(figsize= (15,8))\n","stateholiday_sales=sns.barplot(x='SchoolHoliday',y='Sales',data=merged_df,palette='viridis')\n","plt.title(\"School Holiday Sales\")"],"metadata":{"id":"RGjFvhbbbTcQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"DEMFeE18HO92"}},{"cell_type":"markdown","source":["I picked the specific chart because it is a bar chart. Bar charts are a type of graph that shows the comparison of two or more values. The bars are arranged in a way that makes it easy to see the data  In this chart, the two values are Sales and School Holiday. The bors represent the average sales for each school holiday."],"metadata":{"id":"zyFFR4BgHO92"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"MR3eS7xTHO93"}},{"cell_type":"markdown","source":["We can observe that most of the stores remain closed during State and Holidays. But it is interesting to note that the number of stores opened during School Holidays were more than that were opened during State Holidays. Another important thing to note is that the stores which were opened during School holidays had more sales than normal.\n","\n","1. Sales are significantly higher during school holidays than during non-school holidays.\n","2. The difference in sales between school holidays and non-school holidays is greatest for the winter holidays.\n","3. There is a smaller difference in sales between school holidays and non-school holidays for the summer holidays."],"metadata":{"id":"0neckRpZHO93"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"JmI1Y4YbHO93"}},{"cell_type":"markdown","source":["Bussiness can use this information to decide when to open their store in order to maximaize the sales. For Example a bussiness might decide to open their storeduring the winter holidays in order to take advantage of the increased sales."],"metadata":{"id":"OgeOEDgIHO93"}},{"cell_type":"markdown","source":["#### **Chart - 8**"],"metadata":{"id":"XGShqbS2HSAY"}},{"cell_type":"code","source":["plt.figure(figsize=(15,8))\n","sns.countplot(x=\"DayOfWeek\",hue = \"Promo\", data=merged_df,palette=\"viridis\")\n","plt.title('Store Daily Promo Countplot')"],"metadata":{"id":"-6GTfuViHSAY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"B4XxyvTzHSAZ"}},{"cell_type":"markdown","source":["I picked the specific chart because it is a bar chart. Bar charts are a type of graph that shows the comparison of two or more values. The bars. are arranged in a way that makes it easy to see the data.\n","\n","In this chart, the two values are Sales and Promo. The bars represent the average sales for each promotion."],"metadata":{"id":"xBXFKddDHSAZ"}},{"cell_type":"markdown","source":["##### 2. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"BgP_e3WlHSAZ"}},{"cell_type":"markdown","source":["Stores that run promotions have significantly higher sales than stores that do not run promotions. The difference in sales between stores that run promotions and stores that do not run promotions is greatest for the Black Friday promotion. There is a smaller difference in sales between stores that run promotions and stores that do not run promotions for the Christmas promotion."],"metadata":{"id":"15jESfbyHSAZ"}},{"cell_type":"markdown","source":["#### Chart - 9 -"],"metadata":{"id":"I3aNiIh9fKqZ"}},{"cell_type":"code","source":["sns.scatterplot(x=merged_df['Customers'],y=merged_df['Sales'])"],"metadata":{"id":"WPVINT_UfKqa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"hTF1e9PwfKqa"}},{"cell_type":"markdown","source":["I picked the specific chart because it is scatter plot. Scatter plots are a type of graph that shows the relationship between two variables. The dots are arranged in a way that makes asy to see the data.\n","\n","\n","In this chart, the two variables are Sales and Customers. The dots represent individual stores, and the x-axis represents the Customers in thousands. The y-axis represents the Sales in euros.\n","\n","\n","The chart shows that there is a positive correlation between Sales and Customers. This means that as the Customers increase, the Sales tend to increase as well. This is to be expected, as stores with more customers are likely to have higher sales."],"metadata":{"id":"XOYaZpKpfKqa"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"1xrBLU6IfKqa"}},{"cell_type":"markdown","source":["There is a positive correlation between Sales and Customers, Stores with more customers tend to have higher sales.\n","\n","The impact of customers on sales increases as the number of customers increases\n","\n","There are some outliers in the data, such as the store with the highest sales and the store with the lowest sales."],"metadata":{"id":"rrwjuBlHfKqb"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"yiLHMWFSfKqb"}},{"cell_type":"markdown","source":["Yes, the gained insights can help create a positive business Impact. Businesses can use this information to decide where to locate their stores in order to maximize their sales.\n","\n","For example ess might decide to locate their store in an area with a high concentration of potential customers in order to increase their sales."],"metadata":{"id":"7cTtRDwufKqb"}},{"cell_type":"markdown","source":["#### Chart - 10 -"],"metadata":{"id":"NC_X3p0fY2L0"}},{"cell_type":"code","source":["sns.distplot(x=merged_df['Sales'])"],"metadata":{"id":"xyC9zolEZNRQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"UV0SzAkaZNRQ"}},{"cell_type":"markdown","source":["I picked this chart because it is a good example of how data visualization can be used to identify insights. The density plot shows that the population density is not evenly distributed across the world. Instead, there are a few regions with very high population densities, such as Southeast Asia and India. These regions also have large populations, which means that they are home to a significant portion of the world's population."],"metadata":{"id":"DVPuT8LYZNRQ"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"YPEH6qLeZNRQ"}},{"cell_type":"markdown","source":["1. The world's population is not evenly distributed.\n","\n","2. There are a few regions with very high population densities.\n","\n","3. These regions also have large populations."],"metadata":{"id":"bfSqtnDqZNRR"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact?"],"metadata":{"id":"wShwhtOF-Mcs"}},{"cell_type":"markdown","source":["These insights could help to create a positive business impact in a number of ways. For example, businesses coul arget their marketing campaigns to specific regions with high population densities. They could also open new factories or offices in these regions to take advantage of the large labor pool.\n","\n","There are also some insights that could lead to negative growth. For example, the high population densities in some regions could put a strain on resources such as water and food. This could lead to conflict and instability in these regions.\n","\n","\n","Overall, the density plot is a good example of how data visualization can be used to identify insights. These insights can be used to create a positive business impact, but they also need to be considered carefully to avoid negative consequences."],"metadata":{"id":"oK24XBUB--Wc"}},{"cell_type":"markdown","source":["#### Chart - 11 -"],"metadata":{"id":"q29F0dvdveiT"}},{"cell_type":"code","source":["sns.scatterplot(x=merged_df['CompetitionDistance'],y=merged_df['Sales'])"],"metadata":{"id":"o58-TEIhveiU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"EXh0U9oCveiU"}},{"cell_type":"markdown","source":["I picked the specific chart because it is a scatter chart. Scatter charts are a type of graph that shows the relationship between two variables. The dots are arranged in a way that makes it easy to see the data."],"metadata":{"id":"eMmPjTByveiU"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"22aHeOlLveiV"}},{"cell_type":"markdown","source":["From the above scatter plot it can be observed that mostly the competitor stores weren't that far from each other and the stores densely located near each other saw more sales.\n","\n","\n","In this chart, the two variables are Sales and CompetitionDistance. The dots represent individual stores, and the x-axis represents the CompetitionDistance in meters. The y-ands represents the Sales in euros.\n","\n","\n","The chart shows that there is a negative correlation between Sales and CompetitionDistance. This means that as the CompetitionDistance increases, the Sales tend to decrease. This is to be expected, as stores with more competition are likely to have lower sales"],"metadata":{"id":"uPQ8RGwHveiV"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact?"],"metadata":{"id":"F2NB59MiA5w4"}},{"cell_type":"markdown","source":["I think this chart is interesting because it shows how the number of competitors can affect a store's sales. This information could be useful for businesses that are considering opening a new store. They can use this information to decide where to locate the ore in order to maximize their sales."],"metadata":{"id":"1emCTJRQA8QQ"}},{"cell_type":"markdown","source":["#### Chart - 12 -"],"metadata":{"id":"NQzXqh17CJVE"}},{"cell_type":"code","source":["plt.figure(figsize=(15,10))\n","sns.heatmap(merged_df.corr(),cmap='coolwarm',annot=True)"],"metadata":{"id":"naZFlObwCXao"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"DQ7w8CJfJlA0"}},{"cell_type":"markdown","source":["Correlation is a statistical term used to measure the degree in which two variables move in relation to each other. A perfect positive correlation means that the correlation coefficient is exactly 1. This implies that as one variable moves, either up or down, the other moves in the same direction. A perfect negative correlation means that two variables move in opposite directions, while a zero correlation implies no linear relationship at all.\n"],"metadata":{"id":"WQ4ZndUGJkpY"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"D9TlajBDCW4Q"}},{"cell_type":"markdown","source":["1. Day of the week has a negative correlation indicating low sales as the weekends, and promo, customers and open has positive\n","\n","2. State Holiday has a negative correlation suggesting that stores are mostly closed on state holidays Indicating low sales correlation.\n","\n","3. CompetitionDistance showing negative correlation suggests that as the distance increases sales reduce, which was also observed through the scatterplot earlier.\n","\n","4. There's multicollinearity involved in the dataset as well. The features telling the same story like Promo2, Promo2 since week and year are showing multicollinearity.\n","\n","5. The correlation matrix is agreeing with all the observations done earlier while exploring through barplats and scatterplots."],"metadata":{"id":"xnO7aQ5nFio4"}},{"cell_type":"markdown","source":["#### Chart - 13 -"],"metadata":{"id":"C0ZskiAKKlg8"}},{"cell_type":"code","source":["corr_df=merged_df[merged_df['Sales']>0]\n","sns.pairplot(corr_df[['DayOfWeek','Sales','Customers','Store']])"],"metadata":{"id":"oVyG01QKLB7s"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"2SyfU0i5LBsE"}},{"cell_type":"markdown","source":["The pairplot is a useful visualization tool for the Rossmann sales prediction project because it allows us to quickly examine the relationships between multiple variables in the dataset. In particular, we can use the pairplot."],"metadata":{"id":"_UAdivThLBGk"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"QKpupELlLAsA"}},{"cell_type":"markdown","source":["The chart shows the relationship between the number of customers, sales, and the day of the week for a retail store. There are a few insights that can be found from the chart:\n","\n","⚫ There is a clear relationship between the number of customers and sales. As the number of customers increases, so do sales. This is to be expected, as more customers means more potential for sales.\n","\n","The day of the week also has an impact on sales. On average, sales are highest on Saturdays and Sundays, and lowest on Wednesdays and Thursdays. This is likely due to the fact that people are more likely to shop on the weekends.\n","\n","⚫ There are some outliers in the data. For example, on Saturday, January 26th, there were only 200 customers, but sales were still relatively high. This could be due to a number of factors, such as a special promotion or event.\n","\n","Overall, the chart provides some useful insights into the relationship between the number of customers, sales, and the day of the week for a retail store. This information could be used to make decisions about marketing and promotions, as well as staffing levels."],"metadata":{"id":"G7ZCgBXHLBhI"}},{"cell_type":"markdown","source":["#### **EDA Conclusions and Hypotheses Validation**"],"metadata":{"id":"HnrrUA_hdmk6"}},{"cell_type":"markdown","source":["There's a poorer commation between contomers and sales which is explanatory.\n","\n","Here it can be deduced that there were mor sales on Monday, probably because shops generally remain closed on Sundays which had the lowest sales in a week This validates the hypothesis about this feature.\n","\n","The positive effect of promotion on Customers and Sales is observable.\n","\n","it clear that most of the stores remain closed during State and School Holidays\n","\n","But it is important to note that more streswere opens on School Holidays than on Stat Holidays and her had more sales than State Holidays.\n","\n","Based on the above findings it seems that there are quite a lot of opportunities in store type b&d as they had more number of customers per store and more sales per customer, respectively. Sture type a &c are quite similar in terms of per customer and per sture sales numbers and just because the majority of the stores me of these kinds, they had the best overall nu numbent. On the other hand, store type b were very few in number and even then they had better average sales than others.\n","\n","Earlier, it was observed that only store type b had all three kinds of assortment levels and rest of the store types had two of the it seems that in some type stores the products were different as compared to others because the revenue per store in significantly more Than the others.\n","\n","When comparing the sales of the three years, it is observabite that sales increase by the end of the year indicating that people shop more before the holidays. All the stores showed Chistmas seasonality This validates the pros hypothesis\n","\n","The second thing to notice was that sales dropped for a few months in 2014 accounting for the stores closed due to refurbishment Matt stores have competition distance within the age of 0 to 10 kms and had more sales tan stores far away"],"metadata":{"id":"1sXrZjTzd3Bn"}},{"cell_type":"markdown","source":["## ***5. Feature Engineering & Data Pre-processing***"],"metadata":{"id":"yLjJCtPM0KBk"}},{"cell_type":"code","source":["plt.figure(figsize=(15,8))\n","correlation=merged_df.corr()\n","sns.heatmap(abs(correlation),annot=True,cmap='Reds',linewidths=2,fmt=\".2f\")"],"metadata":{"id":"nEQ5IKIFtrok"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 1. Handling Missing Values"],"metadata":{"id":"xiyOF9F70UgQ"}},{"cell_type":"code","source":["df=merged_df.copy()"],"metadata":{"id":"nxMK7aV61DI0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.shape"],"metadata":{"id":"Xu2B3LQwdkr3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Handling Missing Values & Missing Value Imputation\n","# Missing Values/Null Values Count\n","print(df.isnull().sum())\n","\n","# Visualizing the missing values\n","# Checking Null Value by plotting Heatmap\n","sns.heatmap(df.isnull(), cbar=False)"],"metadata":{"id":"iRsAHk1K0fpS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Out of 1115 entries there are missing values for the columns:\n","\n","• CompetitionDistance-distance in meters to the nearest competitor store, the distribution plot would give us an idea about the distances at which generally the stores are opened and we would impute the values accordingly.\n","\n","• CompetitionOpenSinceMonth-gives the approximate month of the time the nearest competitor was opened, mode of the column would tell us the most occuring month\n","\n","• CompetitionOpenSince Year gives the approximate year of the time the nearest competitor was opened, mode of the column would tell us the most occuring month\n","\n","• Promo2SinceWeek, Promo2Since Year and Promointerval are NaN wherever Promo2 is 0 or False as can be seen in the first look of the dataset. They can be replaced with 0."],"metadata":{"id":"1ixusLtI0pqI"}},{"cell_type":"code","source":["#Step 1: Handling missing values in the \"CompetitionDistance\" column\n","\n","# option 1: Fill missing values with the mean of the column\n","\n","df['CompetitionDistance'].fillna(df['CompetitionDistance'].mean(), inplace=True)\n","\n","#Option 2: Fill missing values with a specific value (e.g., -1 to indicate no competition nearby)\n","\n","#data['CompetitionDistance'].fillna(-1, inplace=True)\n","\n","#Step 2: Handling missing values in \"CompetitionOpenSinceMonth\" and \"CompetitionOpenSinceYear\" columns # We can't directly impute these missing values with meaningful data as the information is not provided.\n","\n","#One option is to fill them with the minimum value for month and year, assuming they started competing right from the beginning\n","\n","df['CompetitionOpenSinceMonth'].fillna(df['CompetitionOpenSinceMonth'].min(), inplace=True)\n","\n","df['CompetitionOpenSinceYear']. fillna(df['CompetitionOpenSinceYear'].min(),inplace=True)\n","\n","# Step 3: Handling missing values in \"Promo25inceweek,\" \"Promo2sinceYear,\" and \"Promo Interval\" columns # We can't directly impute these missing values with meaningful data as the information is not provided.\n","\n","# We can fill them with values like @ or None to indicate that Promo2 is not applicable for these stores.\n","\n","df['Promo2SinceWeek'].fillna(0, inplace=True)\n","\n","df['Promo2SinceYear'].fillna(0, inplace=True)\n","\n","df['PromoInterval'].fillna('None', inplace=True)\n","\n","#Now, the DataFrame \"data\" should have all missing values handled.\n","\n","df.head()"],"metadata":{"id":"fgBLYiWFGksY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### What all missing value imputation techniques have you used and why did you use those techniques?"],"metadata":{"id":"7wuGOrhz0itI"}},{"cell_type":"markdown","source":["Handling missing values is an important step in data pre-processing for any predictive modeling project, including the Rossmann sales prediction project. Missing values can be caused by a variety of reasons, such as incomplete data, data entry errors, or system failures.\n","\n","1. Identify the missing values: The first step is to identify the missing values in the dataset. You can use the isnull() or isna() methods to check for missing values in each column of the dataset.\n","\n","2. Visualize the missing values: Once you have identified the missing values, you can use visualization tools to understand the pattern and extent of missingness. You can use the missingno library to create a heatmap of the missing values.\n","\n","3. Decide on the imputation strategy. There are several strategies for imputing missing values, such as mean imputation, median imputation, mode imputation, and regression imputation. The choice of imputation strategy depends on the nature of the missing values and the characteristics of the dataset.\n","\n","4. Implement the imputation strategy: Once you have decided on the imputation strategy, you can use Python libraries such as sklearn or fancyimpute to implement the imputation strategy.\n","\n","In the above example, we first loaded the dataset and identified the missing values using the isnull() method. We then used the missingno library to visualize the missing values. We decided to use mean imputation for the Competition Distance' variable and used the SimpleImputer class from the sklearn library to perform the imputation. We also showed an alternative method using the KNN function from the fancyimpute library for regression imputation"],"metadata":{"id":"gyT8ndk4KURT"}},{"cell_type":"markdown","source":["### 2. Handling Outliers"],"metadata":{"id":"id1riN9m0vUs"}},{"cell_type":"code","source":["#Identify the outliers\n","\n","# In this example, we will use the box plot to identify the outliers for the 'Sales' variable\n","\n","sns.boxplot(x='Sales', data=merged_df)\n","\n","# Decide on the outlier treatment strategy\n","\n","# In this example, we will winsorize the outliers for the 'Sales' variable\n","\n","merged_df['Sales']=winsorize(merged_df['Sales'], limits=[0.05, 0.05])\n","\n","# Alternatively, you can remove the outliers for the 'Sales' variable\n","\n","q1= merged_df['Sales'].quantile(0.25)\n","q3= merged_df['Sales'].quantile(0.75)\n","\n","iqr = q3 - q1\n","\n","df = merged_df[(merged_df['Sales'] >= q1-1.5*iqr) & (merged_df['Sales'] <= q3 + 1.5*iqr)]"],"metadata":{"id":"M6w2CzZf04JK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### What all outlier treatment techniques have you used and why did you use those techniques?"],"metadata":{"id":"578E2V7j08f6"}},{"cell_type":"markdown","source":["Handling outliers is another important step in feature engineering and data pre processing for the Rossmann sales prediction project. Outliers are data points that are significantly different from the other data points in the dataset and can have a significant impact on the predictive model.\n","\n","Here are some steps for handling outliers and performing outlier treatments in the feature engineering and data pre-processing of data.\n","\n","1. Identify the outliers: The first step is to identify the outliers in the dataset. You can use visualization tools such as box plots, scatter plots, or histograms to identify the outliers.\n","\n","2. Decide on the outlier treatment strategy: There are several strategies for treating outliers, such as removing the outliers, winsorizing the outliers, or transforming the variables using logarithmic or square root transformations.\n","\n","3. Implement the outlier treatment strategy: Once you have decided on the outlier treatment strategy, you can use Python libraries such as numpy, scipy, or sklearn to implement the outlier treatment.\n","\n","In the above example, we first loaded the dataset and used a box plot to identify the outliers for the 'Sales' variable. We then decided to winsorize the outliers for the 'Sales' variable using the winsorize() function from the scipy.stats.mstats module. We also showed an alternative method of removing the outliers using the interquartile range (IQR) method.\n","\n","In the above example, we first loaded the dataset and used a box plot to identify the outliers for the 'Sales' variable. We then decided to winsorize the outliers for the 'Sales' variable using the winsorize() function from the scipy.stats.mstats module. We also showed an alternative method of removing the outliers using the interquartile range (IQR) method.\n","\n","It's important to note that outlier treatment should be performed with caution, as removing or transforming the outliers can also impact the predictive power of the model. It's always a good practice to explore and understand the nature of the outliers and the impact of the outlier treatment on the overall model performance."],"metadata":{"id":"MUP21tsOyJjb"}},{"cell_type":"markdown","source":["### 3. Categorical Encoding"],"metadata":{"id":"89xtkJwZ18nB"}},{"cell_type":"code","source":["# One-Hot Encoding\n","\n","# In this example, we will one hot encode the 'StateHoliday' column\n","\n","df = pd.get_dummies (merged_df, columns=['StateHoliday'])\n","\n","# Alternatively, you can use the LabelEncoder from scikit-learn to perform label encoding\n","\n","le = LabelEncoder()\n","\n","df['StateHoliday'] = le.fit_transform(merged_df['StateHoliday'])"],"metadata":{"id":"21JmIYMG2hEo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.columns"],"metadata":{"id":"ag8Fxggc-IeI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### What all categorical encoding techniques have you used & why did you use those techniques?"],"metadata":{"id":"67NQN5KX2AMe"}},{"cell_type":"markdown","source":["Encoding categorical columns is an important step in feature engineering and data pre-processing for the data. Categorical columns contain values that are not numerical and cannot be used directly in predictive models. Therefore, we need to convert these categorical variables into numerical variables using encoding techniques.\n","\n","Here are some common encoding techniques used for categorical variables:\n","\n","1. Label Encoding: This is a simple technique where each category is assigned a unique numerical label. However, this technique can be problematic when the categories have no inherent order, as the numerical labels can introduce an unintended order to the categories.\n","\n","2. One-Hot Encoding: This technique creates a binary column for each category, indicating whether or not a data point belongs to that category. This technique is useful when the categories have no inherent order and when the number of categories is not too large.\n","\n","3. Binary Encoding: This technique creates binary codes for each category, where each digit represents a power of 2. This technique is useful when the number of categories is large.\n","\n","In the above code, we loaded the dataset and used one-hot encoding to convert the 'StateHoliday' column into numerical columns using the get dummies() function from the pandas module. We also showed an alternative method of label encoding using the LabelEncoder from the sklearn.preprocessing module.\n","\n","It's important to note that the encoding technique used should depend on the nature of the categorical varial and the specific requirements of the predictive model. It's always a good practice to explore and understand the nature of the categorical variables and the impact of the encoding technique on the overall model performance."],"metadata":{"id":"UDaue5h32n_G"}},{"cell_type":"markdown","source":["### 4. Textual Data Preprocessing\n","(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)"],"metadata":{"id":"Iwf50b-R2tYG"}},{"cell_type":"markdown","source":["#### 1. Expand Contraction"],"metadata":{"id":"GMQiZwjn3iu7"}},{"cell_type":"code","source":["def expand_contractions(text):\n","    # Define a contraction mapping\n","    contractions = {\n","        \"ain't\": \"are not\",\n","        \"aren't\": \"are not\",\n","        \"can't\": \"cannot\",\n","        \"couldn't\": \"could not\",\n","        \"didn't\": \"did not\",\n","        \"doesn't\": \"does not\",\n","        \"don't\": \"do not\",\n","        \"hadn't\": \"had not\",\n","        \"hasn't\": \"has not\",\n","        \"haven't\": \"have not\",\n","        \"he's\": \"he is\",\n","        \"I'd\": \"I would\",\n","        \"I'll\": \"I will\",\n","        \"I'm\": \"I am\",\n","        \"I've\": \"I have\",\n","        \"isn't\": \"is not\",\n","        \"it's\": \"it is\",\n","        \"let's\": \"let us\",\n","        \"mustn't\": \"must not\",\n","        \"shan't\": \"shall not\",\n","        \"she's\": \"she is\",\n","        \"shouldn't\": \"should not\",\n","        \"that's\": \"that is\",\n","        \"there's\": \"there is\",\n","        \"they're\": \"they are\",\n","        \"we're\": \"we are\",\n","        \"weren't\": \"were not\",\n","        \"what's\": \"what is\",\n","        \"where's\": \"where is\",\n","        \"who's\": \"who is\",\n","        \"won't\": \"will not\",\n","        \"wouldn't\": \"would not\",\n","        \"you'd\": \"you would\",\n","        \"you'll\": \"you will\",\n","        \"you're\": \"you are\",\n","        \"you've\": \"you have\"\n","    }\n","\n","    # Split the text into words and expand contractions\n","    words = text.split()\n","    expanded_words = [contractions.get(word, word) for word in words]\n","    expanded_text = \" \".join(expanded_words)\n","\n","    return expanded_text\n","\n","# Example usage:\n","text = \"I can't believe he's coming. I'd like to meet him.\"\n","expanded_text = expand_contractions(text)\n","print(expanded_text)"],"metadata":{"id":"PTouz10C3oNN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 2. Lower Casing"],"metadata":{"id":"WVIkgGqN3qsr"}},{"cell_type":"code","source":["# Lower Casing\n","def lowercase_text(text):\n","    # Use the lower() method to convert the text to lowercase\n","    lowercased_text = text.lower()\n","    return lowercased_text\n","\n","# Example usage:\n","text = \"This is an EXAMPLE Text.\"\n","lowercased_text = lowercase_text(text)\n","print(lowercased_text)"],"metadata":{"id":"88JnJ1jN3w7j"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def lower_case(text):\n"," if isinstance(text, str):\n","  return text.lower()\n","\n","# Function to lowercase a single text element return text\n","\n","# Return the original value for non-string elements\n","\n","# Apply lowercase function to the entire DataFrame\n","\n","merged_df = merged_df.applymap(lower_case)\n","\n","merged_df"],"metadata":{"id":"SN9E2DyAVvWC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 3. Removing Punctuations"],"metadata":{"id":"XkPnILGE3zoT"}},{"cell_type":"code","source":["# Remove Punctuations\n","def remove_punctuations(text):\n","  if isinstance(text,str):\n","    return text.replace('[{}]'.format(string.punctuation),'',regex=True)\n","  return text"],"metadata":{"id":"vqbBqNaA33c0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 4. Removing URLs & Removing words and digits contain digits."],"metadata":{"id":"Hlsf0x5436Go"}},{"cell_type":"code","source":["# Remove URLs & Remove words and digits contain digits\n","def remove_urls(text):\n","  return re.sub(r'http\\S+|www\\S+|https\\S+','',text)\n","def remove_words_with_digits(text):\n","  return re.sub(r'\\b\\w*\\d\\w*\\b','',text)\n","\n","data={\n","    'text':[\n","        'This is a test sentence with a URL: https://www.example.com',\n","        'Another sentence  with numbers 123 !',\n","        'Text without any URLs or digits',\n","        'http://urlwithoutwww.com'\n","    ]\n","}\n","test_df=pd.DataFrame(data)\n","test_df['text']=test_df['text'].apply(remove_urls)\n","test_df['text']=test_df['text'].apply(remove_words_with_digits)\n","print(test_df)"],"metadata":{"id":"2sxKgKxu4Ip3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 5. Removing Stopwords & Removing White spaces"],"metadata":{"id":"mT9DMSJo4nBL"}},{"cell_type":"code","source":["# Remove Stopwords\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","stop_words=set(stopwords.words('english'))\n","def remove_stopwords(text):\n","  words=word_tokenize(text)\n","  filtered_words=[word for word in words if word.lower() not in stop_words]\n","  return ' '.join(filtered_words)\n","data={\n","    'text':[\n","        'This is a simple text with some sommon english stopwords',\n","        'Stopwords are common words that are often removed during text analysis',\n","        'we need to remove them to focus on more meaningful words'\n","    ]\n","}\n","test_df=pd.DataFrame(data)\n","test_df['text']=test_df['text'].apply(remove_stopwords)\n","print(test_df)"],"metadata":{"id":"T2LSJh154s8W"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Remove White spaces\n","def remove_white_spaces(text):\n","  if isinstance(text,str):\n","    return ' '.join(text.split())\n","  return text\n","test_df['text']=test_df['text'].apply(remove_white_spaces)\n","print(test_df)"],"metadata":{"id":"EgLJGffy4vm0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 6. Multicollinearity"],"metadata":{"id":"LqcsUX7aOl9Q"}},{"cell_type":"code","source":["df.columns"],"metadata":{"id":"tYV4jn4AdS40"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def calc_vif(X):\n","  vif=pd.DataFrame()\n","  vif[\"Variables\"]=X.columns\n","  vif[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n","  return vif\n","\n","#the_variance_Inflation_Factor_(VIF)\n","test_df=calc_vif(df[[i for i in df.describe().columns if i not in ['Sales']]])\n","print(test_df)"],"metadata":{"id":"V3YFwVb2Ok28"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["In above table we can see that VIF(Variance Inflation Factor) value for column Promo2 and Promo2SienceYear is Higher. So we will drop either Promo2 or Promo2SineceYear and again check VIF value. Here we drop Promo2 column."],"metadata":{"id":"SC1hA-EoXMJi"}},{"cell_type":"code","source":["calc_vif(df[[i for i in df.describe().columns if i not in ['Sales','Promo2']]])"],"metadata":{"id":"_PSZE0X-VMsK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pd.Series(df['Sales'],).hist(bins=5,color=\"green\")\n","plt.show()"],"metadata":{"id":"p9PakPrsbjAD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df[(df.Open == 0)&(df.Sales==0)].count()[0]"],"metadata":{"id":"f7qWOlCZeu7V"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["So we will drop those store which sales is 0 assuming that the stores were closed temporarily and this will help to train the model more accurately."],"metadata":{"id":"aCdvBcfNi2Hp"}},{"cell_type":"code","source":["df1=df.drop(df[(df.Open==0)&(df.Sales==0)].index)"],"metadata":{"id":"meEx1JibgOBe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df1.shape"],"metadata":{"id":"XjgXq7MYjXOW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df1"],"metadata":{"id":"7bxJOxB8jmfe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df1.columns"],"metadata":{"id":"3KSPDdiCkUOw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df1=pd.get_dummies(df1,columns=['PromoInterval'])"],"metadata":{"id":"zqhpQnczj3SO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df1.to_csv('processed_data.csv',index=False)"],"metadata":{"id":"zUED65DDnFxv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## ***6. Hypothesis Testing***"],"metadata":{"id":"g-ATYxFrGrvw"}},{"cell_type":"markdown","source":["### Hypothetical Statement - 1\n","**Model 1 (excluding rows which has 0 sales)**"],"metadata":{"id":"8yEUt7NnHlrM"}},{"cell_type":"markdown","source":["### Hypothetical Statement - 2\n","**Model 2 (By taking whole dataset)**"],"metadata":{"id":"jLrtcv83OEmf"}},{"cell_type":"markdown","source":["## ***7. ML Model Implementation***"],"metadata":{"id":"VfCC591jGiD4"}},{"cell_type":"code","source":["score_df = pd.DataFrame()\n","\n","scoring = make_scorer(f1_score,pos_label=1)\n","\n","features=[i for i in df1.columns if i not in ['Sales']]\n","\n","def analyse_model(model,X_train,X_test,y_train,y_test):\n","  model.fit(X_train,y_train)\n","\n","  try:\n","    try:\n","      importance = model.feature_importances_\n","      feature = features\n","    except:\n","      importance = np.abs(model.coef_[0])\n","      feature = x.columns.values.tolist()\n","    indicies = np.argsort(importance)\n","    indicies = indicies[::-1]\n","  except:\n","    pass\n","\n","  for x,act,label in ((X_train,y_train,'Train_set'),(X_test,y_test,'Test_set')):\n","\n","    # plotting evaluation matrix for train and test dataset\n","    pred=model.predict(x)\n","    pred_proba=model.predict_proba(x)[:,1]\n","    report=pd.DataFrame(classification_report(y_pred=pred,y_true=act,output_dict=True))\n","    fpr,tpr,thresholds = roc_curve(act,pred_proba)\n","\n","    #classification  report\n","    plt.figure(figsize=(18,3))\n","    plt.subplot(1,3,1)\n","    sns.heatmap(report.iloc[:-1,:-1].T,annot=True,cmap='coolwarm')\n","    plt.title(f'{label} Report')\n","\n","    # Confusion matrix\n","    plt.subplot(1,3,2)\n","    sns.heatmap(confusion_matrix(y_true=act,y_pred=pred),annot=True,cmap='coolwarm')\n","    plt.title(f'{label} Report')\n","    plt.xlabel('Predicted labels')\n","\n","    global score_df\n","    score_df[model]={'precision': precision_score(act,pred),'recall': recall_score(act,pred),'f1_score':f1_score(act,pred),'accuracy':accuracy_score(act,pred)}\n","\n","    # AUC_ROC Curve\n","    plt.subplot(1,3,3)\n","    plt.plot([0,1],[0,1],'k--')\n","    plt.plot(fpr,tpr,label=f'AUC={np.round(np.trapz(tpr,fpr),3)}')\n","    plt.legend(loc=4)\n","    plt.title(f'{label} AUC_ROC Curve')\n","    plt.xlabel('False Positive Rate')\n","    plt.ylabel('True Positive Rate')\n","    plt.tight_layout()\n","\n","    #print(\"Importance:\", importance)\n","    #print(\"Feature:\", feature)\n","    #plotting feature immportance\n","  try:\n","    plt.figure(figsize=(18,3))\n","    plt.bar(range(len(indicies)),importance[indicies])\n","    plt.xticks(range(len(indicies)),[feature[i] for i in indicies])\n","    plt.title('Feature Importance')\n","  except:\n","    #print(indicies)\n","    pass\n","  plt.show()\n","\n","  return model"],"metadata":{"id":"Arau8h9vkAoo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### ML Model - 1 - **Implementing Logistic Regression**"],"metadata":{"id":"peAK6Cc_HQeo"}},{"cell_type":"markdown","source":["#### ML Model - 1 Implementation (Exclude rows having 0 sales)"],"metadata":{"id":"Xc5ebVICp_A9"}},{"cell_type":"markdown","source":["As we have two datasets, first one having 0 sales rows and another one excluding it. We will both the data and finf the best model. First we will take dataset excluding 0 sales rows."],"metadata":{"id":"FUhs2uKxqQIQ"}},{"cell_type":"code","metadata":{"id":"xbsRRWRfEQtK"},"source":["# ML Model - 1 Implementation\n","# Fit the Algorithm & Prediction on the model\n","dependent_variables='Sales'\n","independent_variables=list(df1.columns.drop(['Promo2SinceYear','Date','Sales']))"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df1.columns"],"metadata":{"id":"Wgwp_IMt9-BY"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hF36F8oQEVIp"},"source":["independent_variables"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uUsxKejfytOD"},"source":["x=df1[independent_variables]\n","y=df1[dependent_variables]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"H_5ATxY7EbXx"},"source":["X_train,X_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=0)\n","print(X_test.shape)\n","print(X_train.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"biUzo8zBExKg"},"source":["# train the ML Model\n","linear_reg=LinearRegression().fit(X_train,y_train)"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# checking the regression score\n","linear_reg.score(X_train,y_train)"],"metadata":{"id":"rPNV-u-PxNeK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# cheking the coefficient of independent column\n","linear_reg.coef_"],"metadata":{"id":"_yIdHBOnxYuE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# checking the intercept of different column\n","linear_reg.intercept_"],"metadata":{"id":"cFOIHQX6x2Rb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Prediciting on Train Dataset\n","y_pred=linear_reg.predict(X_test)\n","y_pred"],"metadata":{"id":"qCy-0CMayLLK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# actual value\n","y_test"],"metadata":{"id":"hxOGgUTsyYH8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Prediciting on train dataset\n","y_pred_train=linear_reg.predict(X_train)\n","y_pred_train"],"metadata":{"id":"Hajk19p2yh5K"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# train dataset of dependent variable\n","y_train"],"metadata":{"id":"4oPZngKJzSGN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# calculate Mse & Rmse for test Prediction\n","Mse=mse(y_test,y_pred)\n","print(\"MSE value is \",Mse)\n","Rmse=np.sqrt(Mse)\n","print(\"RMSE value is \",Rmse)"],"metadata":{"id":"gf_Rm6RIzR72"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# calculate the R-squared score between the true target values and the predicted values\n","r2=r2_score(y_test,y_pred)\n","print(\"R2 value is\", r2)"],"metadata":{"id":"BQxj5NH4zRog"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_df=pd.DataFrame(zip(y_test,y_pred),columns=['actual','pred'])"],"metadata":{"id":"HjxLs7M21-ve"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### SHAP & LAME"],"metadata":{"id":"Bu9Q7NRZ2lIl"}},{"cell_type":"code","source":["new_df=pd.read_csv(\"/content/processed_data.csv\")"],"metadata":{"id":"GdoINvo81-Zk"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0r2loKjLE9KB"},"source":["# Separate Year Month Day from the Date column\n","new_df['Date']=pd.to_datetime(new_df['Date'])\n","new_df['Year']=new_df['Date'].dt.year\n","new_df['Month']=new_df['Date'].dt.month\n","new_df['DayOfWeek']=new_df['Date'].dt.dayofweek\n","\n","# select relevant features\n","features=['Store','DayOfWeek','Promo','StateHoliday','SchoolHoliday','Year','Month']\n","\n","x=new_df[features]\n","y=new_df['Sales']\n","# Divide the entire dataset in test and train dataset\n","\n","X_train,X_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=42)\n","\n","# Choose the model\n","model=LinearRegression()\n","\n","# train the model\n","model.fit(X_train,y_train)\n","\n","# create a shap exlainner object\n","explain = shap.LinearExplainer(model,X_train,feature_perturbation=\"interventional\")\n","\n","#Calculate the SHAP for best set\n","shap_values = explain.shap_values(X_test)\n","\n","#plot the shap values for the first feature of the first test instance\n","shap.summary_plot(shap_values, X_test, feature_names=X_test.columns)\n","\n","#create Lime explainer object\n","explain=lime.lime_tabular.LimeTabularExplainer(X_train.values,feature_names=X_train.columns, class_names=[\"Sales\"],mode=\"regression\")\n","\n","#explain the prediction model for the best instance using LIME\n","expl=explain.explain_instance(X_test.values[0],model.predict,num_features=4)\n","\n","print(expl.as_list())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### LASSO"],"metadata":{"id":"GNUeDNIzEEyw"}},{"cell_type":"code","metadata":{"id":"_dUfuIPtFBi7"},"source":["lasso_1=Lasso(alpha=0.4,max_iter=10000,selection='cyclic',tol=0.0001)\n","\n","# Trainig the model\n","lasso_1.fit(X_train,y_train)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"J_O81exKFNUx"},"source":["# after training the model predicted the test dataset\n","y_estimate_lasso=lasso_1.predict(X_test)\n","\n","lasso_1.score(X_test,y_test)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LuG-5ThU-9w5"},"source":["score=cross_val_score(lasso_1,x,y,cv=10)\n","print(score)\n","\n","mean_score=score.mean()\n","print(\"mean Score is:\",mean_score)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"f1py3kXS_XDw"},"source":["# range of alpha values to be tested\n","para={'alpha':[0.1,0.2,0.3,0.4,0.5]}\n","\n","# Perform gridsearchcv for optimum alpha value\n","lasso_1_cross=GridSearchCV(lasso_1,para,cv=5)\n","lasso_1_cross.fit(new_df[features],new_df['Sales'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["optimum_alpha=lasso_1_cross.best_params_['alpha']\n","optimum_score=lasso_1_cross.best_score_\n","print(\"Best alpha value for the model & optimum accuracy\",optimum_alpha,optimum_score)"],"metadata":{"id":"nNVvbV7f1Gh4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pd.DataFrame(zip(y_test,y_estimate_lasso),columns=['actual','pred'])"],"metadata":{"id":"JhTqoAuQ1Gel"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["lasso_2=Ridge(alpha=0.5)\n","\n","lasso_2.fit(X_train,y_train)"],"metadata":{"id":"ihJlioa31Ga1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y_estimate=lasso_2.predict(X_test)\n","pd.DataFrame(zip(y_test,y_estimate),columns=['actual','pred'])"],"metadata":{"id":"ZNyzR80V1GSl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["lasso_2.score(X_test,y_test)"],"metadata":{"id":"amMAXTwF3eOH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ridge=Ridge(max_iter=10000,solver='auto')\n","\n","#define the alpha value\n","para={'alpha':[0.1,0.2,0.3,0.4,0.5]}\n","\n","ridge_cv=GridSearchCV(lasso_2,para,cv=5)\n","ridge_cv.fit(x,y)\n","\n","best_alpha=ridge_cv.best_params_['alpha']\n","best_score=ridge_cv.best_score_\n","\n","ridge_best= Ridge(alpha=best_alpha,max_iter=10000,solver='auto')\n","cv_scores=cross_val_score(ridge_best,x,y,cv=5)\n","\n","score_max=cv_scores.max()\n","max_alpha=best_alpha\n","\n","print(\"Best score: \",best_score)\n","print(\"Maximum CV Score: \",score_max)\n","print(\"Corresponding alpha value: \",best_alpha)"],"metadata":{"id":"ozPHjpeG3eL4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Elastic Net"],"metadata":{"id":"efvLEhhi-A3z"}},{"cell_type":"code","source":["elastic_net=ElasticNet(max_iter=10000)\n","\n","#define the alpha value\n","para={'alpha':[0.1,0.2,0.3,0.4,0.5],'l1_ratio':[0.1,0.2,0.3,0.4,0.5]}\n","\n","elastic_net_cv=GridSearchCV(elastic_net,para,cv=5)\n","elastic_net_cv.fit(X_train,y_train)\n","\n","best_alpha=elastic_net_cv.best_params_['alpha']\n","best_l1_score=elastic_net_cv.best_params_['l1_ratio']\n","best_score=elastic_net_cv.best_score_\n","\n","elastic_net_best= ElasticNet(alpha=best_alpha,l1_ratio=best_l1_score,max_iter=10000)\n","elastic_net_best.fit(X_train,y_train)\n","\n","test_score=elastic_net_best.score(X_test,y_test)\n","\n","print(\"Best score: \",best_score)\n","print(\"Maximum CV Score: \",score_max)\n","print(\"Corresponding alpha value: \",best_alpha)\n","print(\"Corresponding l1 ratio value: \",best_l1_score)"],"metadata":{"id":"Di0WOlpV3eJT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Decision Tree"],"metadata":{"id":"e00cEby84KTE"}},{"cell_type":"code","source":["sales_mean=y.mean()\n","print(sales_mean)\n"],"metadata":{"id":"HB6Nq4MT3eGn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sales_mean_new=new_df['Sales'].mean()\n","print(sales_mean_new)"],"metadata":{"id":"B3C-Bv9u3eEF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### ML Model - 2 - **By Taking Whole  Dataset**"],"metadata":{"id":"dJ2tPlVmpsJ0"}},{"cell_type":"code","source":["df1.columns"],"metadata":{"id":"ULgBUAmTpsJ1"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BNzv3F70d3Fb"},"source":["dependent_df=df1['Sales']\n","\n","independent_df=df1.drop(columns=['Store','Promo2SinceYear','Date','Sales'])\n","\n","D=dependent_df.values\n","\n","I=independent_df.values"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["independent_df.shape"],"metadata":{"id":"93zy3QBgF_5M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df1.drop(columns=['Store','Promo2SinceYear','Date','Sales']).values"],"metadata":{"id":"p_WlvwK68uY0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_train,X_test,y_train,y_test=train_test_split(independent_df,dependent_df,test_size=0.2,random_state=0)\n","print(X_train.shape)\n","print(X_test.shape)"],"metadata":{"id":"xDYEA70I_F4I"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Linear Regression"],"metadata":{"id":"Xd1XC66-AjrQ"}},{"cell_type":"code","source":["# scaling the independent values\n","scaler=StandardScaler()\n","X_train=scaler.fit_transform(X_train)\n","X_test=scaler.fit_transform(X_test)\n","\n","#train the linear regression model\n","linear_reg_1=LinearRegression()\n","linear_reg_1.fit(X_train,y_train)\n","\n","# use this model to predict on test dataset\n","y_pred=linear_reg_1.predict(X_test)\n","print(y_pred)\n","\n","# calculating the score\n","linear_reg_1.score(X_train,y_train)\n","\n","#storing actual and predicted value in a dataframe\n","reg_df=pd.DataFrame(zip(y_test,y_pred),columns=['actual','pred'])\n","reg_df"],"metadata":{"id":"aOYXwUiT_IUY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# caculating and printing the MSE value\n","MSE=mse(y_test,y_pred)\n","print(\"MSE Value is: \",MSE)\n","\n","RMSE=np.sqrt(MSE)\n","print(\"RMSE value is: \",RMSE)\n","\n","r22=r2_score(y_test,y_pred)\n","print(\"R2: \",r22)"],"metadata":{"id":"VFFSkb2k_IRQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Decision Tree"],"metadata":{"id":"dZfbiuRDNiGI"}},{"cell_type":"code","source":["decision_tree=DecisionTreeRegressor(max_depth=5)\n","decision_tree.fit(X_train,y_train)\n","\n","y_pred=decision_tree.predict(X_test)\n","y_train=decision_tree.predict(X_train)\n","\n","MSE=mse(y_test,y_pred)\n","print(\"MSE: \",MSE)\n","\n","rmse=np.sqrt(MSE)\n","print(\"RMSE value is: \",rmse)\n","\n","r2=r2_score(y_test,y_pred)\n","print(\"r2 scors is: \",r2)"],"metadata":{"id":"kmA4fa21_IOI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["decision_tree_df=pd.DataFrame(zip(y_test,y_pred),columns=['actual','predicted'])\n","print(decision_tree_df)"],"metadata":{"id":"-gZa4Ukk_ILA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Random Forest Classifier"],"metadata":{"id":"wm2rIru5Rmyg"}},{"cell_type":"code","source":["random_forest=RandomForestRegressor(n_estimators=500,max_depth=8,n_jobs=2)\n","random_forest.fit(X_train,y_train)\n","\n","y_pred=random_forest.predict(X_test)\n","y_train=random_forest.predict(X_train)\n","\n","MSE=mse(y_test,y_pred)\n","print(\"MSE: \",MSE  )\n","\n","rmse=np.sqrt(MSE)\n","print(\"RMSE value is: \",rmse)\n","\n","r2=r2_score(y_test,y_pred)\n","print(\"r2 scors is: \",r2)"],"metadata":{"id":"bj1ljqJN_IGU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Ada BoostRegressor"],"metadata":{"id":"9Bsy5AUnUYR4"}},{"cell_type":"code","source":["ada_boost=AdaBoostRegressor(n_estimators=500,learning_rate=0.01)\n","ada_boost.fit(X_train,y_train)\n","\n","y_pred=ada_boost.predict(X_test)\n","y_train=ada_boost.predict(X_train)\n","\n","\n","MSE=mse(y_test,y_pred)\n","print(\"MSE: \",MSE)\n","\n","rmse=np.sqrt(MSE)\n","print(\"RMSE value is: \",rmse)\n","\n","r2=r2_score(y_test,y_pred)\n","print(\"r2 scors is: \",r2)\n"],"metadata":{"id":"bomucqTM_H88"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### XGBoost Regressior Model"],"metadata":{"id":"1UxMW878VXfM"}},{"cell_type":"code","source":["xgboost=XGBRegressor(n_estimators=500,learning_rate=0.01)\n","xgboost.fit(X_train,y_train)\n","\n","y_pred=xgboostpredict(X_test)\n","y_train=xgboost.predict(X_train)\n","\n","MSE=mse(y_test,y_pred)\n","print(\"MSE: \",MSE)\n","\n","rmse=np.sqrt(MSE)\n","print(\"RMSE value is: \",rmse)\n","\n","r2=r2_score(y_test,y_pred)\n","print(\"r2 scors is: \",r2)"],"metadata":{"id":"9a1lUUGpVW_8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Conclusion**"],"metadata":{"id":"gCX9965dhzqZ"}},{"cell_type":"markdown","source":["We saw that Sales column contains 172817 rows with 0 sale. So we created a new dataframe in which we removed 0 sales rows and tried to train our model. We used various algorithms and got accuracy score around 74%.\n","• We were also curious about the total dataset(including Sales = 0 rows). So we trained another model using various algorithms and we got accuracy near about 98% which is far better than previous model.\n","• So we came to conclusion that removing sales=0 rows actually removes lot of information from dataset as it has 172817 rows which is quite large and therefore we decided not to remove those values. We got our best rmpse score from Random Forest model, Graident boosting technique like adaboost,xgboost,we tried taking an optimum parameter so that our model doesnt overfit."],"metadata":{"id":"Fjb1IsQkh3yE"}}]}